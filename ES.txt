PUT 更新操作

Meta-Fields(元数据)


ES分片[shards ]：文档划分成多份，这些份叫分片（允许水平分割扩展，进行分布并行操作提高性能吞吐量）;
replicas 副本：上述分片的一份或多份的拷贝.（为了提高可用性副本，复制分片从不与原/主要（original/primary）分片置于同一节点上）
默认5个主分片1份副本

搜索引擎的核心是倒排索引，而倒排索引的基础就是分词
ES中处理分词的部分叫作分词器Analyzer
ES的分词不仅仅发生成文档创建时 也发生成搜索时（读时分词（处理写入请求）和写时分词（处理读取请求））
ES 自带了很多默认的分词器，比如Standard、Keyword、Whitespace等等，默认是Standard。当我们在读时或者写时分词时可以指定要使用的分词器

写时分词器需要在 mapping 中指定，而且一经指定就不能再修改，若要修改必须新建索引

ES分片修改，MAPPING需要修改 都需要reindex
复制数据重建索引
POST http://ip:9200/_reindex
{
  "source": {
    "index": "old_index"
  },
  "dest": {
    "index": "new_index"
  }
}

设置别名
POST /_aliases
{
      "actions": [
          {"add": {"index": "my_index2", "alias": "my_index"}}
      ]
}


1.创建带mapping设置的索引
PUT http://ip:9200/indexname
{
	"mappings":{
		"docType":{
			"properties":{
				"filedOne":{
					"type":"keyword"
					"analyzer":"standard"
				}
			}
		}
	}
}

 
创建索引并指定设置分片及mappings信息
PUT http://ip:9200/indexName
{
 "settings":{
 		"number_of_shards":5,
 		"number_of_replicas":1
 },
 "mappings":{
 		"docTypeName":{
 			"properties":{
 				"name":{
 					"type":"text"
 				}
 				"age":{
 					"type":"integer"
 				}
 			}
 		}
 }
}

先关闭index后再更新settings（仅能修改副本数，索引分片数在索引创建好了之后就不能调整了，只能重建索引）
PUT http://ip:9200/indexName/_settings
{
	"number_of_replicas":0
}

查看ES 各索引情况（后加?v显示列头项）
GET http://172.16.100.122:9200/_cat/indices?v

查看集群健康状态
GET http://ip:port:/_cat/health?v

查询获取集群结点列表
GET http://ip:9200/_cat/nodes?v

查看index对应的_mapping及_setting配置信息
GET http://ip:9200/indexName

查看index对应的setting配置信息
GET http://172.16.100.122:9200/indexName/_settings

查看对应index的mapping配置信息
GET http://172.16.100.122:9200/indexName/_mapping
GET http://172.16.100.122:9200/indexName/_mappings

删除index
DELETE http://ip:9200/indexName
DELETE http://ip:9200/indexOne,indexTwo
DELETE http://ip:9200/index*
DELETE http://ip:9200/_all

无法删除文档类型
DELETE http://ip:9200/index/doc/id  //删除指定文档



文档更新(ES更新实际是先删除再新增的过程)
POST http://ip:9200/indexName/typeName/id/_update?pretty
{
 "doc":{
   "field1":"value1", //更新字段值
   "field2":"xxx"     //还可以新增一些字段及值
   .....
 }
}


关闭/打开 index
POST http://ip:9200/indexName/_open
POST http://ip:9200/indexName/_close


查询文档
post http://ip:9200/indexName/_search
{
 "query":{
 	"match":{
 		"fieldname":"value"
 	}
 }
}






创建索引文件时，不指定ID，会自生成一个随机ID
指定已存在的ID时会覆盖更新原文档内容，不存在自动以此ID创建文档







ES-SQL插件

修改ES 字段类型：https://www.cnblogs.com/hxlasky/p/10175607.html

curl -XPOST http://172.16.67.3:24148/idx_cust_val/doc -H 'Content-Type: application/json' -d '{"s_CATR020001":"1001610007","s_CATR020002":"王明月","s_CATR010004":"32010219910202537","s_CATR010003":"14","s_CATR070003":"410101","s_CATR010017":"410101","s_CATR070001":"200104","s_CATR070004":"天津银行成都分行","s_CATR010019":"天津银行成都分行","s_CATR010002":"","s_CATR070006":"","s_CATR010015":"","s_CATR070002":"","s_CATR010018":"","s_CATR070005":"","s_CATR010001":"","s_CATR010014":"","s_CATR010008":"","w_CATR030004":"","w_CATR010002":"","w_CATR070002":"","w_CATR010001":"","w_CATR030002":"","w_CATR170001":"","w_CATR180004":"奥术大师大所多,阿达水电费,私有客户标签,理财标签","w_CATR070005":"","w_CATR180002":"160000,160001,160002,160003","w_CATR070004":"","i_CVAL100002":2,"i_CVAL100005":14,"f_CVAL200023":52214.4,"f_CVAL200010":96480.39,"f_CVAL100002":99748.94,"f_CVAL200044":26761.33,"f_CVAL200022":13491.91,"f_CVAL200011":58496.17,"f_CVAL090065":96920.11,"f_CVAL200040":85387.87,"f_CVAL200047":63213.52,"f_CVAL200001":51560.19,"f_CVAL100028":19408.08,"f_CVAL200008":13640.26,"f_CVAL100014":60986.67,"f_CVAL200021":90018.75,"f_CVAL090003":90775.86,"f_CVAL200014":43189.06,"f_CVAL090057":73732.17,"f_CVAL200038":90571.4,"f_CVAL200050":3130.75,"f_CVAL200004":16205.47,"f_CVAL100022":97360.74,"f_CVAL100020":89718.98,"f_CVAL100016":13292.26,"f_CVAL200017":32159.57,"f_CVAL090061":46649.02,"f_CVAL090002":65245.53,"f_CVAL200030":15259.82,"f_CVAL200002":99226.9,"f_CVAL090058":87250.82,"f_CVAL200036":67240.49,"f_CVAL200048":61962.08,"f_CVAL100026":50207.24,"f_CVAL100006":31418.63,"f_CVAL200020":67524.97,"f_CVAL090066":67353.54,"f_CVAL200032":42070.17,"f_CVAL200029":12946.17,"f_CVAL090004":55171.38,"f_CVAL200005":91419.98,"f_CVAL090059":97305.77,"f_CVAL200039":51088.1,"f_CVAL200046":43438.59,"f_CVAL090001":37515.79,"f_CVAL200018":96335.18,"f_CVAL090062":29580.31,"f_CVAL200035":29035.13,"f_CVAL200042":98761.58,"f_CVAL200028":25467.16,"f_CVAL200003":19679.06,"f_CVAL100008":83762.87,"f_CVAL200049":5589.4,"f_CVAL200027":72731.19,"f_CVAL200016":44009.98,"f_CVAL090055":15302.98,"f_CVAL200033":61889.18,"f_CVAL200045":42736.64,"f_CVAL200006":18700.59,"f_CVAL100027":10208.14,"f_CVAL100021":39657.41,"f_CVAL100010":98838.58,"f_CVAL200012":22857.29,"f_CVAL200026":75624.73,"f_CVAL200025":16249.91,"f_CVAL200019":65567.35,"f_CVAL090060":2494.99,"f_CVAL200031":73108.55,"f_CVAL200043":6820.67,"f_CVAL200009":29522.78,"f_CVAL100025":8092.14,"f_CVAL100023":27540.52,"f_CVAL100018":33013.85,"f_CVAL100004":11217.15,"f_CVAL200015":53937.01,"f_CVAL090063":2177.11,"f_CVAL200024":26765.07,"f_CVAL090006":7569.25,"f_CVAL200007":97106.87,"f_CVAL090056":47029.46,"f_CVAL200034":89554.17,"f_CVAL200041":42883.91,"f_CVAL140001":43484.02,"f_CVAL100024":30055.32,"f_CVAL100012":19362.69,"f_CVAL200013":77509.69,"f_CVAL090064":60315.87,"f_CVAL200037":80036.45,"f_CVAL090005":93302.16}'



kibana: http://172.16.100.122:5601  DELETE /biplog* 删除索引
elasticsearch-head http://172.16.100.122:9100/
springboot elk https://segmentfault.com/a/1190000015606497?utm_source=tag-newest


qwx572511-3
@!zsGhIATM0&


172.16.100.122
es安装目录：/opt/elasticSearch
/opt/elasticSearch/elasticsearch-6.8.0/bin




实例项目：https://gitee.com/52itstyle/spring-boot-elasticsearch

https://blog.csdn.net/weixin_34190136/article/details/91446546
ES基于apache lucene之上的开源分布式搜素引擎(elasticsearch-head是一个界面化的集群操作和管理工具)
ES还是一个分布式文档数据库（非结构化 的数据库），其中每个字段均可被索引，而且每个字段的数据均可被搜索，能够横向扩展至数以百计的服务器存储以及处理PB级的数据
近实时（NRT）
ES是一个近实时的搜索引擎（平台），代表着从添加数据到能被搜索到只有很少的延迟。（大约是1s）

索引:索引是具有某种相似特性的文档集合
文档：一个文档是一个可被索引的基础信息单元


正向索引是通过文档去查找单词，
倒排索引（反向索引)则是通过单词去查找文档
(正排索引：文档id到单词的关联关系;倒排索引：单词到文档id的关联关系)


#创建索引
curl -X PUT "localhost:9200/customer"
#创建文档（添加数据）
curl -X PUT "localhost:9200/customer/_doc/1" -H 'Content-Type: application/json' -d'{  "name": "John Doe"}'
#查询文档（查询数据）
curl -X GET "localhost:9200/customer/_doc/1"
#删除文档（删除数据）
curl -X DELETE "localhost:9200/customer"

95589











Elasticsearch 与 Solr 的比较总结
二者安装都很简单；
Solr 利用 Zookeeper 进行分布式管理，而 Elasticsearch 自身带有分布式协调管理功能;
Solr 支持更多格式的数据，而 Elasticsearch 仅支持json文件格式；
Solr 官方提供的功能更多，而 Elasticsearch 本身更注重于核心功能，高级功能多有第三方插件提供；
Solr 在传统的搜索应用中表现好于 Elasticsearch，但在处理实时搜索应用时效率明显低于 Elasticsearch。
Solr 是传统搜索应用的有力解决方案，但 Elasticsearch 更适用于新兴的实时搜索应用。

其他基于Lucene的开源搜索引擎解决方案*
直接使用 Lucene
说明：Lucene 是一个 JAVA 搜索类库，它本身并不是一个完整的解决方案，需要额外的开发工作。

kibana:https://www.cnblogs.com/cjsblog/p/9476813.html
Logstash介绍 https://www.cnblogs.com/cjsblog/p/9459781.html

CAP理论就是说在分布式存储系统中，最多只能实现上面的两点。而由于网络硬件肯定会出现延迟丢包等问题，所以分区容错性是我们必须需要实现的



https://www.elastic.co/cn/downloads/elasticsearch

elasticsearch / 123456
B-Tree索引(mysql)
B+Tree

https://gitee.com/52itstyle/spring-boot-elasticsearch

kibana :https://artifacts.elastic.co/downloads/kibana/kibana-6.8.0-linux-x86_64.tar.gz


Elasticsearch可视化插件head

Elasticsearch安装(需要先安装JDK)
1.下载安装包：wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.4.2-linux-x86_64.tar.gz

2.解压 tar -zxf elasticsearch-6.8.0.tar.gz

3.需要新建elasticsearch组及用户，用以来启动
# 以root用户来创建新的用户 ， groupadd 添加一个用户组
[root@localhost home]# groupadd elasticsearch 
# 添加一个用户，-g是在用户组下 -p是密码
[root@localhost home]# useradd elasticsearch -g elasticsearch -p elasticsearch
# 进入es的安装目录
[root@localhost home]# cd /home/elasticsearch 
# 给用户elasticsearch 授权 修改文件拥有者
[root@localhost home]# chown -R elasticsearch:elasticsearch elasticsearch-6.1.2/
# 切换到 elasticsearch 用户
[root@localhost elasticsearch]# su elasticsearch

4.修改IP Port
vi config/elasticsearch.yml

4.1修改其他相关配置：
使用ROOT用户修改
vi /etc/security/limits.conf

修改线程文件数量 指定到elasticsearch用户，用“*” 则匹配所有用户(通过ulimit -Hn查看)
elasticsearch hard nofile 65536 //每个进程最大同时打开文件数
elasticsearch soft nofile 65536

elasticsearch soft nproc 4096 //最大线程个数
elasticsearch hard nproc 4096

修改虚机最大内存/etc/sysctl.conf文件，增加配置vm.max_map_count=262144
vi /etc/sysctl.conf
sysctl -p

5.启动
一定要先切换成elasticsearch用户 再启动！！！！
 ./bin/elasticsearch
后台启动 加参数-d ./bin/elasticsearch -d
 ./bin/elasticsearch -d && tail -f logs/elasticsearch.log

./elasticsearch -Ecluster.name=my_cluster_name -Enode.name=my_node_name //通过Ecluster.name Enode.name参数重命名集群及结点名称
 

6.查看进程jps
7.检查elasticsearch运行情况：curl http://localhost:9200/
查看ES是否启动：
ps -ef | grep elasticsearch

8.ES相关命令
查看集群健康
curl -X GET "172.16.100.122:9200/_cat/health?v"

查看集群结点列表
curl -X GET "172.16.100.122:9200/_cat/nodes?v"

安装IK分词器 https://github.com/medcl/elasticsearch-analysis-ik
在线安装：
1.执行命令：./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.8.0/elasticsearch-analysis-ik-6.8.0.zip
2.离线解压安装：
https://github.com/medcl/elasticsearch-analysis-ik/releases 下地最新分词器并解压文件至es安装目录plugins目录下，重启ES即可

IK分词模式
ik_max_word:将文本最细粒度拆分
ik_smart：对文本做粗粒度拆分

网上关于两种分词器使用的最佳实践是：索引时用ik_max_word，在搜索时用ik_smart。
即：索引时最大化的将文章内容分词，搜索时更精确的搜索到想要的结果。

分词测试
post _analyze
{
  "analyzer":"ik_max_word"
  "text":"这是测试文本"
}

自定义个人字典：
/opt/elasticSearch/elasticsearch-6.8.0/config/analysis-ik
IKAnalyzer.cfg.xml


=========================================================================
安装elasticsearch-head
elasticsearch-head是一个用于浏览ElasticSearch集群并与其进行交互的Web项目
1.yum -y install nodejs npm
2.yum -y install git
3.git clone https://github.com/mobz/elasticsearch-head.git
4.克隆下GIT项目后进行编译处理
cd elasticsearch-head/
npm install --registry=https://registry.npm.taobao.org

5.修改elasticsearch.yml，增加跨域的配置(需要重启es才能生效)
vi /etc/elasticsearch/elasticsearch.yml
增加如下内容
http.cors.enabled: true
http.cors.allow-origin: "*"

6.修改Gruntfile.js文件，修改服务监听地址（增加hostname属性，将其值设置为*）
connect:{
  server:{
  	options:{
  		hostname:'*',
  		port:9100
  	}
  }
}

7.修改head/_site/app.js，修改head连接es的地址（修改localhost为本机的IP地址）
this.base_uri=this.config.base_uri||this.prefs.get("app-base_uri")||"172.16.100.122:9200";

8.启动插件
elasticsearch-head安装目录下执行
npm run start &

9.访问
http://172.16.100.122:9100/

备注：
#使用淘宝的镜像源来安装grunt程序，grunt是基于nodejs的项目构建工具，可以进行打包压缩、测试、执行等工作，head插件就是通过grunt启动。
npm install -g grunt --registry=https://registry.npm.taobao.org

#安装grunt工具的客户端
npm install -g grunt-cli --registry=https://registry.npm.taobao.org 


================================================================

安装kibana( Kibana 是为 Elasticsearch设计的开源分析和可视化平台)
1.下载安装包：wget https://artifacts.elastic.co/downloads/kibana/kibana-6.8.0-linux-x86_64.tar.gz
2.解压tar -zxf kibana-6.8.0-linux-x86_64.tar.gz
3.修改安装目录config/kibana.yml
4.启动命令./bin/kibana &
5.查看进程：ps -ef|grep node
或：netstat -tunlp|grep 5601


----------------------------------------------
logstash安装(特别教程：http://doc.yonyoucloud.com/doc/logstash-best-practice-cn/get_start/index.html)
1.下载wget https://artifacts.elastic.co/downloads/logstash/logstash-6.8.0.tar.gz
2.解压
tar -zxvf logstash-2.3.1.tar.gz -C /bigdata/

3.修改配置
vi logstash.conf
input {
	file {
		type => "gamelog"
		path => "/log/*/*.log"
		discover_interval => 10
		start_position => "beginning" 
	}
}
 
output {
    elasticsearch {
		index => "gamelog-%{+YYYY.MM.dd}"
        hosts => ["172.16.0.14:9200", "172.16.0.15:9200", "172.16.0.16:9200"]
    }
}

说明：
#discover_interval=> 15  //设置多长时间扫描目录，发现新文件单位MS
#stat_interval=> 1 //设置多长时间检测文件是否修改 默认为1MS

4.启动：
bin/logstash -f logstash.conf
或
后台启动 nohup bin/logstash -f config/kafka.conf & tail -f nohup.out

直接启动：
bin/logstash -e 'input { stdin {} } output { stdout{} }'

bin/logstash -e 'input { stdin {} } output { stdout{codec => rubydebug} }'

bin/logstash -e 'input { stdin {} } output { elasticsearch {hosts => ["172.16.0.14:9200"]} stdout{} }'

bin/logstash -e 'input { stdin {} } output { elasticsearch {hosts => ["172.16.0.15:9200", "172.16.0.16:9200"]} stdout{} }'

bin/logstash -e 'input { stdin {} } output { kafka { topic_id => "test" bootstrap_servers => "172.16.0.11:9092,172.16.0.12:9092,172.16.0.13:9092"} stdout{codec => rubydebug} }'


脚本方式后台启动
vim /opt/elogstash/logstash-6.5.1/startup.sh
nohup bin/logstash -f config/kafka.conf &
./startup.sh 即可后台启动
tail -f nohup.out 可查看启动日志

ELK日志系统 搭建教程：http://www.imooc.com/article/266484
logstash 开源的日志收集引擎，具备实时传输的能力

logstash配置示例
input {
        file {
                type => "biplog"
                path => "/opt/tj_bip/nohup.out"
                discover_interval => 10
                start_position => "beginning" 
        }
}
 
output {
    elasticsearch {
                index => "biplog-%{+YYYY.MM.dd}"
        hosts => ["172.16.100.122:9200"]
    }
}


=========================================================
elasticsearch 之 _mapping映射
1.映射（mapping）相当于数据库表的表结构
2.ES中的映射用来定义一个文档，可以定义文档所包含的字段及字段类型，分词器及属性等等
3.ES映射分为动态映射（文档写入ElasticSearch时，会根据文档字段自动识别类型，这种机制称之为动态映射）及静态映射（事先定义好映射，包含文档的各个字段及其类型等，这种方式称之为静态映射）
4.Near Realtime (NRT)近乎实时的搜索平台，从索引文档到可以搜索的时间只有轻微的延迟（通常是1秒）
5.Cluster：集群是一个或多个节点(服务器)的集合，它们共同保存你的整个数据，并提供跨所有节点的联合索引和搜索功能
6.Node：节点是一个单独的服务器，它是集群的一部分，存储数据，并参与集群的索引和搜索功能
7.index :索引是具有某种相似特征的文档的集合
8.Document:文档是可以被索引的基本信息单元。文档用JSON表示
9.Shards & Replicas分片及副本[分片好处：允许水平分割扩展，允许跨分片（可能在多个结点）分布和并行操作，提高性能及吞吐量]
副本：高可用性，扩展的搜索吞吐量，搜索可以在所有副本上并行执行。

10.Elasticsearch 中数据最重要的三要素当属：索引、类型、文档
10.1索引：
 10.1.1settings是修改分片(number_of_shards)和副本数(number_of_replicas)的。
 10.1.2mappings是修改字段和类型的。
 mapping中的字段类型一旦设置，禁止直接修改，因为 lucene实现的倒排索引生成后不允许修改，应该重新建立新的索引，然后做reindex操作
 
 
检查ES是否在运行：
curl http://localhost:9200/

查看ES集群健康:
curl -X GET "localhost:9200/_cat/health?v"

查看ES结点列表：
curl -X GET "localhost:9200/_cat/nodes?v"

查看ES所有索引
curl -X GET "localhost:9200/_cat/indices?v"

创建一个索引
curl -X PUT "localhost:9200/index名称?pretty"  //创建一个名称为index名称 的索引并以漂亮的JSON格式返回

创建一个索引并添加一个映射
curl -X PUT "localhost:9200/indexName" -H 'Content-Type:application/json' -d
'{
  mappings:{
    "dynamic":false  # false表示在写入文档时，如果写入字段不存在也不会报错。
    "_doc":{
       "properties":{
       		"fieldName":{
       			"type":"text",
       			"index":true, #index参数作用是控制当前字段是否被索引，默认为true，false表示不记录，即不可被搜索
       			"null_value":"" # 这个参数的作用是当字段遇到null值的时候的处理策略，默认为null，即空值，此时es会忽略该值。可以通过这个参数设置某个字段的默认值
       		},
       }
    }
  }
}'
或
PUT my_index
{
  mappings:{
    
  }
}


索引一个文档
curl -X PUT "172.16.100.122:9200/fang-test/test/1000?pretty" -H 'Content-Type:application/json' -d '{"name":"fanghj","job":"it"}'
或
PUT /indexName/_doc/id 
{
 "name":"fanghj",
 "job":"it"
}

查询写入文档：
GET /indexName/_doc/_search
{
 "query":{
   "match":{
   		"name":"fanghj"
   }
 }
}
或者
curl -X GET "172.16.100.122:9200/fang-test/test/_search?pretty" -H 'Content-Type:application/json' -d '{"query":{"match":{"name":"fanghj"}}}'


查看mapping:
GET /[index_name]/_mapping





备注：-X参数指定 HTTP 请求的方法。
wget命令用来从指定的URL下载文件

查看索引配置settings信息
GET /索引名/_settings?pretty
或 curl -X GET "locahost:9200/customer/_settins?pretty"



说明：
?v是用来要求在结果中返回表头
?pretty要求返回一个漂亮的json结果

===============================================================================
ElasticSearch
一、批量导入数据
curl -X POST 'localhost:9200/bank/account/_bulk?pretty' --data-binary @accounts.json

注意：
1 需要在accounts.json所在的目录运行curl命令。
2 localhost:9200是ES得访问地址和端口
3 bank是索引的名称
4 account是类型的名称
5 索引和类型的名称在文件中如果有定义，可以省略；如果没有则必须要指定
6 _bulk是rest得命令，可以批量执行多个操作（操作是在json文件中定义的，原理可以参考之前的翻译）
7 pretty是将返回的信息以可读的JSON形式返回。

二、搜索
ES提供了两种搜索的方式：请求参数方式 和 请求体方式。
curl 'localhost:9200/bank/_search?q=*&pretty' //请求参数方式

curl -X POST 'localhost:9200/bank/_search?pretty' -d  //请求体方式
'{
 "query":{
 	 "match_all":{},
 	 "size":1 //不指定默认返回10条
 	 
 }
}'

首先看一下 _search API 的基本用法：
1. GET /_search?q=查询条件 ： 代表按查询条件在所有索引进行查询
2. GET /索引名称/__search?q=查询条件 ： 代表指定索引进行查询
3. GET /索引1,索引2/__search?q=查询条件 ： 代表指定多个索引进行查询
4. GET /索引*/__search?q=查询条件 ： 通过通配符指定多个索引进行查询

三、查询语言DSL（domain specific language）
1.指定索引查询（具体索引名｜多个索引名用逗号分隔｜可以带星号通配符）
2.可分页查询 size(每页大小),from（从第几个结果开始往后返回,默认为0）
3.

GET /index/_search
{
 "query":{
 		"match":{
 			"name":"x"
 		}
 }
}


{
    "bool": {
        "must": { "match":   { "email": "business opportunity" }},
        "should": [
            { "match":       { "starred": true }},
            { "bool": {
                "must":      { "match": { "folder": "inbox" }},
                "must_not":  { "match": { "spam": true }}
            }}
        ],
        "minimum_should_match": 1
    }
}





四、ES mapping相关属性

0."dynamic": false,  # false表示在写入文档时，如果写入字段不存在也不会报错。

1."_source":{
  "enabled":false, //默认会返回_source查询结果，false后则不再返回_source（禁用_source）
  "includes":["fields1","fields2"],  //指定返回字段返回
  "excludes":["fields"] //排除字段返回
}

2."_all":{
   "enabled":true //默认关闭false
   //_all字段默认是关闭的，如果要开启_all字段，索引增大是不言而喻的。_all字段开启适用于不指定搜索某一个字段，根据关键词，搜索整个文档内容。 
   //或者通过具体fields中include_in_all中开启true|false关闭
}  


3."fileds": {  //具体字段下属性
         "type":  "text", //文本类型  
         
         "index": "analyzed"//分词，不分词是：not_analyzed ，设置成false，字段将不会被索引  
         
         "analyzer":"ik"//指定分词器  
         
         "boost":1.23//字段级别的分数加权  
         
         "doc_values":false//对not_analyzed字段，默认都是开启，analyzed字段不能使用，对排序和聚合能提升较大性能，节约内存,如果您确定不需要对字段进行排序或聚合，或者从script访问字段值，则可以禁用doc值以节省磁盘空间：
         
         "fielddata":{"loading" : "eager" }//Elasticsearch 加载内存 fielddata 的默认行为是 延迟 加载 。 当 Elasticsearch 第一次查询某个字段时，它将会完整加载这个字段所有 Segment 中的倒排索引到内存中，以便于以后的查询能够获取更好的性能。
         
         "fields":{"keyword": {"type": "keyword","ignore_above": 256}} //可以对一个字段提供多种索引模式，同一个字段的值，一个分词，一个不分词  
         
         "ignore_above":100 //超过100个字符的文本，将会被忽略，不被索引
           
         "include_in_all":ture//设置是否此字段包含在_all字段中，默认是true，除非index设置成no选项  
         
         "index_options":"docs"//4个可选参数docs（索引文档号） ,freqs（文档号+词频），positions（文档号+词频+位置，通常用来距离查询），offsets（文档号+词频+位置+偏移量，通常被使用在高亮字段）分词字段默认是position，其他的默认是docs  
         
         "norms":{"enable":true,"loading":"lazy"}//分词字段默认配置，不分词字段：默认{"enable":false}，存储长度因子和索引时boost，建议对需要参与评分字段使用 ，会额外增加内存消耗量  
         
         "null_value":"NULL"//设置一些缺失字段的初始化值，只有string可以使用，分词字段的null值也会被分词  
         
         "position_increament_gap":0//影响距离查询或近似查询，可以设置在多值字段的数据上火分词字段上，查询时可指定slop间隔，默认值是100  
         
         "store":false//是否单独设置此字段的是否存储而从_source字段中分离，默认是false，只能搜索，不能获取值  
         
         "search_analyzer":"ik"//设置搜索时的分词器，默认跟ananlyzer是一致的，比如index时用standard+ngram，搜索时用standard用来完成自动提示功能  
         
         "similarity":"BM25"//默认是TF/IDF算法，指定一个字段评分策略，仅仅对字符串型和分词类型有效  
         
         "term_vector":"no"//默认不存储向量信息，支持参数yes（term存储），with_positions（term+位置）,with_offsets（term+偏移量），with_positions_offsets(term+位置+偏移量) 对快速高亮fast vector highlighter能提升性能，但开启又会加大索引体积，不适合大数据量用  
       }  

doc_values: true|false
index:no|not_analyzed|analyzed
store:no|true 属性用于指定是否将原始字段写入索引，默认取值为no


============================================================
elasticsearch核心概念 https://segmentfault.com/a/1190000016659841
1.索引（索引名不能包含大些字母）创建,删除（DELETE /index-name），打开，关闭，查看配置信息
2.类型
3.文档 
创建指定ID（PUT /index/type/1 {文档数据} ）或者不指定文档ID POST /INDEX/TYPE {文档数据}
ElasticSearch引擎使用Upsert（更新或增加）方式更新索引，这意味着，如果索引中已经存在相同ID的文档，那么ElasticSearch更新该文档（实际上是先删除，后添加）；如果索引中不存在相同ID的文档，那么把文档添加索引中
可以通过增加参数op_type=create 来指定新增
如下两种方式
PUT 'http://localhost:9200/twitter/tweet/1?op_type=create' -d {}
PUT 'http://localhost:9200/twitter/tweet/1/_create' -d {}

1.ES乐观并发控制(通过文档_version)
操作后面增加version=verID&version_type=external来控制





查看文档 GET /index/doc/文档ID（获取文档ID文档信息）  
删除文档 DELETE /index/doc/文档ID
文档的更新 实际上是先删除后新增 POST|PUT /INDEX/TYPE/docID {"K":"V"}
一、文档搜索
<1>检索所有文档 
GET /index/_search 

<2>term查询 查看指定字段内容是否精确匹配输入(term查询用于查找指定字段中包含指定分词的文件，只有当查询分词和文档中的分词精确匹配时才被检索到。)
GET /index/_search
{
	"query":{
		"term":{
			"title":"java"
		}
	}
}

<3>terms查询
GET /index/_search
{
	"query":{
		"terms":{
			"filedName":["c++","java"]
		}
	}
}

<4>match查询 对于match查询，只要被查询字段中存在任何一个词项被匹配，就会搜索到该文档
GET /index/_search
{
	"query":{
		"match":{
			"filedName":"xxx"
		}
	}
}
二、更新文档
<1>更新文档
POST /index/type/docId {"k":"v"}

doc关键字覆盖新增更新文档如下：
post /index/type/docid
{
	"doc":{
		"key1":"value1",
		"key2":"value2"
	}
}
//覆盖更新docid对应文档，字段一样覆盖，字段不一样进行新增

script关键字更新
POST /index/type/docId/_upate
{
	"script":{
		"source": "ctx._source.category=params.category",
    "lang":"painless",
    "params":{"category":"git"}
	}
}


<2>更新文档字段
post /index/type/docId/_update 
{"script":"ctx._source.updatefieldName=\"xxxx\""}

<3>添加文档字段
post /index/type/docid/_update 
{
	"script":"ctx._source.addfieldName=\"xxxx\""
}

<4>查询更新文档
POST blog/_update_by_query
{
  "script": {
    "source": "ctx._source.category=params.category",
    "lang":"painless",
    "params":{"category":"git"}
  },
  "query":{
    "term": {"title":"git"}
  }
}

备注：upsert参数，当指定的文档不存在时，upsert参数包含的内容将会被插入到索引中，作为一个新文档；如果指定的文档存在，ElasticSearch引擎将会执行指定的更新逻辑。
POST /index/type/docid/_update?retry_on_conflict=5
{
	"script":"ctx._source.view=1",
	"upsert":{
		"view":1
	}
}
//如果docid不存在，则创建文档且文档内容为view=1，否则执行更新操作
//_update?retry_on_conflict=5 指定更新操作在发生版本冲突时重试的次数。

<5>删除文档
DELETE /index/type/docId

_update更新文档


<6>查询批量脚本更新_update_by_query
POST /book/_update_by_query
{
  "script": {
    "source": "ctx._source.author=params.author",
    "lang":"painless",
    "params":{"author":"tset"}
  },
  "query":{
    "match": {"bookName":"FUK"}
  }
}

POST /book/type/docid/_update?retry_on_conflict=5
{
	"script":"ctx._source.views+=1",
	"upsert":{
		"views":0
	}
}


<7>_bulk批量处理
POST /_bulk?pretty  --data-binary  request_body //请求主动体，有四种类型，分别是index、update、create和delete，实现数据的索引分析，文档更新，文档创建和文档删除。

向_source字段，增加一个字段
POST'localhost:9200/test/type1/1/_update'-d'{"script":"ctx._source.name_of_new_field = \"value_of_new_field\""}'
从_source字段中，删除一个字段
POST'localhost:9200/test/type1/1/_update'-d'{"script":"ctx._source.remove(\"name_of_field\")"}'




4.分片
5.副本







======================================================================================================
Elasticsearch重要知识点
1.ES是分布式的，使用文档的_version采用乐观并发控制来确保程序修改数据冲突不会造成数据丢失。
即在对文档进行增删改（ES中修改是先删除后再重新新增）操作时，增加参数?version=1&version_type=external来
确保操作之前文档数据未被其他请求修改。且只有当前文档当前_version比指定的version数值小时，如果请求成功，
那么外部版本号会被存入到文档中的_version中。
备注：version_type=external 指定使用外部版本号

2.ES引擎使用upsert（更新或增加）方式更新索引；这意味着如果索引中已经存在相同ID的文档，ES更新文档（实际上是先删除后增加）；
如果索引中不存相再ID的文档则进行新增（把文档增加了索引中去）。
所以在将文档编入索引存在以下几个方式：
	文档新增
	文档更新
	文档删除
2.1 指定DOCID,新增或更新(一定要新增的话需要指定操作类型?op_type=create或PUT /index/type/docid/_create)
PUT /index/type/docId -d {"key":"value"}

2.2 不指定文档ID(ES自动生成文档标识,且会把op_type自动改为creat)
POST /index/type/ -d {"k":"v"}   

2.3 指定操作类型为create
PUT /index/type/1?opt_type=create -d {}
PUT /index/type/1/_create

2.4 删除文档
DELETE /index/doc/docid 

2.5 使用"doc"或"script"字段对文档局部数据更新
POST /index/type/docid/_update -d 
{
 "doc":{
 
 }
 //"script":{}
}

2.6 retry_on_conflict参数，可以指定更新操作在发生版本冲突时重试次数

2.7 向文档中增加删除字段
POST /index/type/docid/_update 
{
	"script":"ctx._source.newFieldName=\"xxx\""
}

POST /index/type/docid/_update
{
	"script":"ctx._source.remove(\"fileName\")"
}

2.8 _bulk批量操作
POST /_bulk?pretty --data-binary request body

3.elasticsearch mapping创建
PUT /index
{
  "settings":{
  	"number_of_shards":5,
  	"number_of_replicas":1
  },
  "mappings":{
  	"typeName":{
  		"properties":{
  			"fieldName":{
  				 "type":"text|integer|date"
  				 .....
  			}
  		}
  	}
  }
}   


Term与Match区别：
　　Term查询不会对字段进行分词查询，会采用精确匹配。
　　Match会根据该字段的分词器，进行分词查询。


ES是什么
elasticsearch简写es，es是一个高扩展、开源的全文检索和分析引擎，它可以准实时地快速存储、搜索、分析海量的数据。

什么是全文检索
全文检索是指计算机索引程序通过扫描文章中的每一个词，对每一个词建立一个索引，指明该词在文章中出现的次数和位置，
当用户查询时，检索程序就根据事先建立的索引进行查找，并将查找的结果反馈给用户的检索方式。这个过程类似于通过字典中的检索字表查字的过程。
全文搜索搜索引擎数据库中的数据。                    

SpringBoot整合ES时 需要注意连接端口
注意java的es默认连接端口是9300，9200是http端口   
注: 9300 是 Java 客户端的端口。9200 是支持 Restful HTTP 的接口。  

Deprecated        

-----------------------------------------------------------------
java调用elasticsearch方式
官方提供两种
1.TransportClient ES7.0版本已经废弃
2.Rest client （http方式兼容所有ES版本）
 2.1java low level rest client
 2.2java hight level rest client
<dependency>
    <groupId>org.elasticsearch.client</groupId>
    <artifactId>elasticsearch-rest-client</artifactId>
    <version>6.8.0</version>
</dependency>

3.springboot集成方式(spring data elasticsearch)
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-elasticsearch</artifactId>
</dependency>

application.xml
spring.data.elasticsearch.cluster-name=my-application
spring.data.elasticsearch.cluster-nodes=127.0.0.1:9300 //注意 9300是Java客户端的端口，9200是支持Restful HTTP的接口

3.1 ElasticsearchTemplate
3.2 ElasticsearchRepository
---------------------------------------------------------------------------------
SpringBoot默认使用两种技术来和ES交互
1、Jest（Jest 是一个Java 版的ElasticSearch Http Rest 客户端，基于HttpClient 封装实现。）
　　需要导入jest的工具包（io.searchbox.client.JestClient）
Jest开源GIT地址：https://github.com/searchbox-io/Jest/tree/master/jest

2、SpringData ElasticSearch
　　1）、Client 节点 ：配置clusterNodes、clusterName
　　2）、ElasticSearchTemplate 操作es
　　3）、编写ElasticSearchRepository的子接口来操作es
2.配置ElasticSearch
spring.elasticsearch.jest.uris=http://tanghu.tk:9200
           